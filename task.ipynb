{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "sms = pd.read_csv('SMSSpamCollection.csv', encoding='latin-1')\n",
    "sms.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, ' MK17 92H. 450Ppw 16\"', ' why to miss them', 'GE',\n",
       "       'U NO THECD ISV.IMPORTANT TOME 4 2MORO\\\\\"\"',\n",
       "       'i wil tolerat.bcs ur my someone..... But',\n",
       "       ' ILLSPEAK 2 U2MORO WEN IM NOT ASLEEP...\\\\\"\"',\n",
       "       'whoever is the KING\\\\\"!... Gud nyt\"', ' TX 4 FONIN HON',\n",
       "       ' \\\\\"OH No! COMPETITION\\\\\". Who knew', 'IåÕL CALL U\\\\\"\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking unnamed columns\n",
    "sms['Unnamed: 3'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seems like unnamed columns have parts of the messages that somehow were wrongly divided in csv.file. \n",
    "* Though it may affect classifier, decision is to drop them for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnecessary columns\n",
    "sms1 = sms.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "sms1 = sms1.rename(columns={\"v1\":\"class\", \"v2\":\"message\"})\n",
    "sms1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at classes %\n",
    "sms1['class'].value_counts() / len(sms1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at classes counts\n",
    "x=pd.value_counts(sms1[\"class\"], sort= True)\n",
    "x.plot(kind= 'bar', color= [\"grey\"])\n",
    "plt.title('Classes distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Metrics choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though receiving spam can be annoying, I think that situation, when actual message is classified as spam, can lead to worse consequenses: situations, when a person can miss important info or meeting just because he/she never checks spam folder.\n",
    "\n",
    "So, the classifier should be focused on predicting ham correctly. If spam is predicted as 1, I need lower False positive rate and can allow for higher False negative rate = I need higher Precision\n",
    "\n",
    "Main metric will be **precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  By hands Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    tokenized=re.split('\\W+',text)\n",
    "    tokenized_no_punctuation=[word.lower() for word in tokenized if word not in string.punctuation]\n",
    "    tokenized_no_stopwords=[word for word in tokenized_no_punctuation if word not in stopwords.words('english')]\n",
    "    tokens = [PorterStemmer().stem(word) for word in tokenized_no_stopwords if word != '️']\n",
    "    return tokens\n",
    "\n",
    "sms1['tokens']=sms1['message'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                            message  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [go, jurong, point, crazi, avail, bugi, n, gre...  \n",
       "1                       [ok, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "4       [nah, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check tokenization\n",
    "sms1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels for prediction\n",
    "# spam = 1, ham = 0\n",
    "sms1['spamlabel']=(sms1['class'] =='spam').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spamlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, think, goe, usf, live, around, though]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                            message  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                              tokens  spamlabel  \n",
       "0  [go, jurong, point, crazi, avail, bugi, n, gre...          0  \n",
       "1                       [ok, lar, joke, wif, u, oni]          0  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...          1  \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]          0  \n",
       "4       [nah, think, goe, usf, live, around, though]          0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "sms1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split\n",
    "index_train, index_test  = train_test_split(np.array(sms1.index), train_size=0.7, random_state=1)\n",
    "train = sms1.loc[index_train,:].copy()\n",
    "test =  sms1.loc[index_test,:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u       0.144\n",
       "go      0.080\n",
       "get     0.069\n",
       "come    0.057\n",
       "ok      0.057\n",
       "call    0.056\n",
       "2       0.055\n",
       "gt      0.050\n",
       "lt      0.050\n",
       "know    0.048\n",
       "like    0.047\n",
       "got     0.046\n",
       "good    0.046\n",
       "day     0.045\n",
       "time    0.045\n",
       "love    0.043\n",
       "want    0.042\n",
       "ur      0.040\n",
       "4       0.037\n",
       "need    0.035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequent words in ham\n",
    "ham =len(sms1[sms1['class']=='ham'])\n",
    "\n",
    "words_ham = nltk.FreqDist()\n",
    "for words in sms1[sms1['class']=='ham']['tokens']:\n",
    "    for word in np.unique(words):\n",
    "            words_ham[word] += 1\n",
    "            \n",
    "freq_words_ham = pd.Series(dict(words_ham))/ham\n",
    "freq_words_ham = freq_words_ham.sort_values(ascending=False)\n",
    "freq_words_ham.head(20).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "call     0.447\n",
       "å        0.320\n",
       "free     0.228\n",
       "txt      0.216\n",
       "2        0.209\n",
       "u        0.174\n",
       "4        0.170\n",
       "text     0.167\n",
       "mobil    0.162\n",
       "ur       0.153\n",
       "claim    0.146\n",
       "1        0.142\n",
       "stop     0.135\n",
       "repli    0.133\n",
       "www      0.131\n",
       "get      0.118\n",
       "prize    0.115\n",
       "uk       0.098\n",
       "min      0.096\n",
       "150p     0.095\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequent words in spam\n",
    "spammessage =len(sms1[sms1['class']=='spam'])\n",
    "\n",
    "words_spam = nltk.FreqDist()\n",
    "for words in sms1[sms1['class']=='spam']['tokens']:\n",
    "    for word in np.unique(words):\n",
    "            words_spam[word] += 1\n",
    "            \n",
    "freq_words_spam = pd.Series(dict(words_spam))/spammessage\n",
    "freq_words_spam = freq_words_spam.sort_values(ascending=False)\n",
    "freq_words_spam.head(20).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13564102564102565\n",
      "0.8643589743589744\n"
     ]
    }
   ],
   "source": [
    "# probability of message to be spam\n",
    "message_in_spam = train['class'].value_counts()['spam'] / len(train)\n",
    "# probability of message to be ham\n",
    "message_in_ham = 1 - message_in_spam\n",
    "\n",
    "print (message_in_spam)\n",
    "print (message_in_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# probability word in spam\n",
    "def pspam(word):\n",
    "    count = 0\n",
    "    total_spam = train['class'].value_counts()['spam']\n",
    "    for index, row in train.iterrows():\n",
    "            if row['class']=='spam' and word in row['tokens']:\n",
    "                count += 1  \n",
    "    return count/total_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22117202268431002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pspam('free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability word in spam correction for zero probability\n",
    "def pspamzerocorr(word):\n",
    "    if pspam(word) != 0:\n",
    "        p = pspam(word)\n",
    "    else: p = 1\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pspamzerocorr('lvblb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability word in ham\n",
    "def pham(word):\n",
    "    count = 0\n",
    "    ttl_ham = train['class'].value_counts()['ham']\n",
    "    for index, row in train.iterrows():\n",
    "            if row['class']=='ham' and word in row['tokens']:\n",
    "                count += 1\n",
    "    return count/ttl_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011865915158706615"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pham ('free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability word in ham correction for zero probability\n",
    "def phamzerocorr(word):\n",
    "    if pham(word) != 0:\n",
    "        ph=pham(word)\n",
    "    else: ph = 1\n",
    "    return ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability message is spam given a word\n",
    "def condprobability(word):\n",
    "        proba = (pspamzerocorr(word)*message_in_spam)/((pspamzerocorr(word)*message_in_spam + phamzerocorr(word)*message_in_ham))\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that a message is spam given the word \"free\" is: 0.7452229299363057\n"
     ]
    }
   ],
   "source": [
    "# let's see:\n",
    "word = 'free'\n",
    "print('Probability that a message is spam given the word \"{}\" is: {}'.format(word, condprobability(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One word can be an unreliable source of info and lead to misclassification, so classifier won't be tested on test sample, I proceed to multiple word classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple-words Naive Bayes & 5-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to classify multiple words\n",
    "def classify(message):\n",
    "    p_spam_given_message = message_in_spam\n",
    "    p_ham_given_message = message_in_ham\n",
    "    for word in message:\n",
    "        p_spam_given_message *= pspamzerocorr(word)\n",
    "        p_ham_given_message *= phamzerocorr(word)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(['i', 'sell', 'pizza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(['find', 'my', 'socks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply predictions to test set\n",
    "test['predicted'] = test['tokens'].apply(classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function wasn't executed for more than an hour - probably, bug in the code (too many loops or smth) but I couldn't debug it so I couldn't fit the model\n",
    "\n",
    "So, next steps would have been **5-fold** cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying cross-validation\n",
    "kf = KFold(n_splits=5, random_state=1, shuffle=False)\n",
    "kf.get_n_splits(sms1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1115 1116 1117 ... 5569 5570 5571] TEST: [   0    1    2 ... 1112 1113 1114]\n",
      "TRAIN: [   0    1    2 ... 5569 5570 5571] TEST: [1115 1116 1117 ... 2227 2228 2229]\n",
      "TRAIN: [   0    1    2 ... 5569 5570 5571] TEST: [2230 2231 2232 ... 3341 3342 3343]\n",
      "TRAIN: [   0    1    2 ... 5569 5570 5571] TEST: [3344 3345 3346 ... 4455 4456 4457]\n",
      "TRAIN: [   0    1    2 ... 4455 4456 4457] TEST: [4458 4459 4460 ... 5569 5570 5571]\n"
     ]
    }
   ],
   "source": [
    "#split\n",
    "for train_index, test_index in kf.split(sms1):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = sms1['tokens'][train_index], sms1['tokens'][test_index]\n",
    "    y_train, y_test = sms1['class'][train_index], sms1['class'][test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exctract using indices\n",
    "train_1 = pd.DataFrame([data['class'][1115:5571], data['tokens'][1115:5571]]).transpose()\n",
    "test_1 = pd.DataFrame([data['class'][0:1114], data['tokens'][0:1114]]).transpose()\n",
    "\n",
    "train_2 = pd.DataFrame([data['class'][0:5571], data['tokens'][0:5571]]).transpose()\n",
    "test_2 = pd.DataFrame([data['class'][1115:2229], data['tokens'][1115:2229]]).transpose()\n",
    "\n",
    "train_3 = pd.DataFrame([data['class'][0:5571], data['tokens'][0:5571]]).transpose()\n",
    "test_3 = pd.DataFrame([data['class'][2230:3343], data['tokens'][2230:3343]]).transpose()\n",
    "\n",
    "train_4 = pd.DataFrame([data['class'][0:5571], data['tokens'][0:5571]]).transpose()\n",
    "test_4 = pd.DataFrame([data['class'][3344:4457], data['tokens'][3344:4457]]).transpose()\n",
    "\n",
    "train_5 = pd.DataFrame([data['class'][0:4457], data['tokens'][0:4457]]).transpose()\n",
    "test_5 = pd.DataFrame([data['class'][4458:5571], data['tokens'][4458:5571]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying predictions to test sets\n",
    "test_1['predicted'] = test_1['tokens'].apply(classify)\n",
    "test_2['predicted'] = test_2['tokens'].apply(classify)\n",
    "test_3['predicted'] = test_3['tokens'].apply(classify)\n",
    "test_4['predicted'] = test_4['tokens'].apply(classify)\n",
    "test_5['predicted'] = test_5['tokens'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating precision\n",
    "precision_1 = precision_score(test_1['class'], test_1['predicted'], average=None)\n",
    "precision_2 = precision_score(test_2['class'], test_2['predicted'], average=None)\n",
    "precision_3 = precision_score(test_3['class'], test_3['predicted'], average=None)\n",
    "precision_4 = precision_score(test_4['class'], test_4['predicted'], average=None)\n",
    "precision_5 = precision_score(test_5['class'], test_5['predicted'], average=None)\n",
    "\n",
    "# getting avg\n",
    "precision_avg = sum(precision_1,precision_1,precision_1,precision_1,precision_1)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sk_learn Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms1['spamlabel']=(sms1['class'] =='spam').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dependent var & regressor\n",
    "x = sms1['message']\n",
    "y = sms1['spamlabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming X_train\n",
    "vect = CountVectorizer(stop_words = 'english')\n",
    "X_train_matrix = vect.fit_transform(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying Multinomial NB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(X_train_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1194\n",
      "           1       0.95      0.91      0.93       199\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.97      0.95      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "X_test_matrix = vect.transform(X_test) \n",
    "predicted_result=clf.predict(X_test_matrix)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predicted_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation precision scores:[0.87804878 0.87719298 0.88983051 0.8974359  0.9122807 ]\n"
     ]
    }
   ],
   "source": [
    "# 5-fold validation\n",
    "scores = cross_val_score(clf, X_train_matrix, y_train, cv = 5, scoring='precision')\n",
    "print('Cross-validation precision scores:{}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score: 0.8910\n"
     ]
    }
   ],
   "source": [
    "print('Average cross-validation score: {:.4f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though in confusion matrix precision is at 97%, based on cross-validation we can say that avg precision score is around 89%. It even can be as low as 87% = for the purposes of avoiding misclassification of normal letters as spam this model should be improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final comments**: as I couldn't test by hands metric on test set I can't compare th results. I can conclude that in-built functions results can be improved by building classifiers through feature extraction. Probably by hand classifier allows for even more tuning and can achieve same or better results than a built-in one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
